{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75d461b-2b8b-44ee-9aee-56639a7db734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/python3 -m pip install --upgrade pip\n",
    "# !pip install -e ..\n",
    "# !pip install nvidia-cudnn-cu11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119bb5a2-b9d3-47fe-8586-3aadcbf27396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from midistral.audio_analysis import (\n",
    "    genre_classes,\n",
    "    get_chords,\n",
    "    get_mood_and_genre,\n",
    "    instruments_classes,\n",
    "    mood_classes,\n",
    ")\n",
    "from midistral.midi_utils import get_instruments, get_midi_and_ogg_from_abc\n",
    "from midistral.types import AudioTextDescription, InferenceApproach\n",
    "from midistral.audio_analysis import SIMPLIFIED_MOODS, SIMPLIFIED_GENRES, get_simplified_genres, get_simplified_moods\n",
    "from midistral.infer import generate_abc_notation\n",
    "from sklearn.metrics import classification_report\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f028a62-c382-4fff-a29a-ac5f769784c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(genres: List[str], labels: List[str]):\n",
    "    v = [0] * len(labels)\n",
    "    for g in genres:\n",
    "        index = labels.index(g)\n",
    "        v[index] = 1\n",
    "    return v\n",
    "\n",
    "\n",
    "def evaluate_prediction(\n",
    "    labels: List[str],\n",
    "    predicted: List[List[str]],\n",
    "    ground_truth: List[List[str]],\n",
    "    output_dict: bool = True,\n",
    "):\n",
    "    predicted_vector = []\n",
    "    ground_truth_vector = []\n",
    "    for pg in predicted:\n",
    "        predicted_vector.append(get_vector(pg, labels))\n",
    "    for gtg in ground_truth:\n",
    "        ground_truth_vector.append(get_vector(gtg, labels))\n",
    "\n",
    "    y_true = np.array(ground_truth_vector)\n",
    "    y_pred = np.array(predicted_vector)\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=labels, output_dict=output_dict\n",
    "    )\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e3930b-ab31-4a29-a960-ce9c0dd7a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOKS_FOLDER = Path(os.getcwd())\n",
    "OUTPUT_FOLDER = NOTEBOOKS_FOLDER.parent / \"output\"\n",
    "DATA_FOLDER = NOTEBOOKS_FOLDER.parent / \"data\"\n",
    "\n",
    "num_samples_by_constraint_p = (\n",
    "    OUTPUT_FOLDER / \"datasets\" / \"num_samples_by_constraint.json\"\n",
    ")\n",
    "TEST_CASES = []\n",
    "with num_samples_by_constraint_p.open(\"r\") as f:\n",
    "    constraints_split = json.load(f)\n",
    "    TEST_CASES.extend(\n",
    "        [\n",
    "            c[\"constraints\"]\n",
    "            for c in constraints_split\n",
    "            if len(c[\"constraints\"][\"genre\"]) > 0\n",
    "            or len(c[\"constraints\"][\"mood\"]) > 0\n",
    "            or len(c[\"constraints\"][\"instrument_summary\"]) > 0\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # TEST_CASES = [{\"instrument_summary\": [\"trumpet\"], \"mood\": [\"calm\"], \"genre\": [\"emotional\"]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2a894-3759-4670-bb9a-9caa054a3442",
   "metadata": {},
   "outputs": [],
   "source": [
    "for approach in [InferenceApproach.FINETUNED_2]:\n",
    "# for approach in [InferenceApproach.PROMPT_ONLY, InferenceApproach.RAG]:\n",
    "    ANNOTATION_OUTPUT_PATH = OUTPUT_FOLDER / f\"annotations_{approach.value}_output.jsonl\"\n",
    "    TMP_AUDIO_FOLDER = OUTPUT_FOLDER / \"tmp_audio\" / approach.value\n",
    "    TMP_AUDIO_FOLDER.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    with ANNOTATION_OUTPUT_PATH.open('a', encoding='utf-8') as annotation_f:\n",
    "        for ind, r in enumerate(tqdm(TEST_CASES)):\n",
    "            instrument_summary_gt = [i.lower() for i in r[\"instrument_summary\"]]\n",
    "\n",
    "            # run inference\n",
    "            # abc_notation_text = \"X: 1\\nM: 4/4\\nL: 1/8\\nQ:1/4=120\\nK:D\\nV:1\\n%%MIDI program 0\\n G/2G/2c/2A/2| B/2B/2d/2G/2| A/2A/2F/2G/2| B/2B/2d/2G/2|G/2G/2c/2A/2| B/2B/2d/2G/2| A/2A/2F/2G/2| B/2B/2d/2G/2|G/2G/2c/2A/2| B/2B/2d/2G/2| A/2A/2F/2G/2| B/2B/2d/2G/2| B/2B/2d/2G/2| A/2A/2F/2G/2| B/2B/2d/2G/2| B/2B/2d/2G/2| A/2A/2F/2G/2| B/2B/2d/2G/2|\\n\"\n",
    "            des = AudioTextDescription(genre=r[\"genre\"], mood=r[\"mood\"], instruments=instrument_summary_gt, midi_instruments_num=None)\n",
    "            abc_notation_text, text_description = generate_abc_notation(des, approach)\n",
    "\n",
    "            # generate audio\n",
    "            try:\n",
    "                midi, ogg = get_midi_and_ogg_from_abc(abc_notation_text)\n",
    "                file_uuid = str(uuid.uuid4())\n",
    "                for extension, b in [(\"midi\", midi), (\"ogg\", ogg)]:\n",
    "                    p = TMP_AUDIO_FOLDER / f\"{file_uuid}.{extension}\"\n",
    "                    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    with p.open(\"wb\") as f:\n",
    "                        f.write(b)\n",
    "\n",
    "                # analyse it\n",
    "                chords_out, chord_summary, chord_summary_occurence = get_chords(TMP_AUDIO_FOLDER / f\"{file_uuid}.ogg\")\n",
    "                try:\n",
    "                    mood_tags, mood_cs, genre_tags, genre_cs = get_mood_and_genre(\n",
    "                        TMP_AUDIO_FOLDER / f\"{file_uuid}.ogg\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    mood_tags, mood_cs, genre_tags, genre_cs = [], [], [], []\n",
    "    \n",
    "                try:\n",
    "                    instrument_numbers_sorted, instrument_summary = get_instruments(\n",
    "                        TMP_AUDIO_FOLDER / f\"{file_uuid}.midi\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    instrument_numbers_sorted, instrument_summary = [], []\n",
    "            except Exception:\n",
    "                file_uuid = None\n",
    "                mood_tags, mood_cs, genre_tags, genre_cs = [], [], [], []\n",
    "                instrument_numbers_sorted, instrument_summary = [], []\n",
    "\n",
    "            # log it\n",
    "            row = {\n",
    "                \"file_uuids\": file_uuid,\n",
    "                \"abc_notation_texts\": abc_notation_text,\n",
    "                \"text_descriptions\": text_description,\n",
    "                \"mood_preds\": mood_tags,\n",
    "                \"mood_cs\": mood_cs,\n",
    "                \"simplified_mood_preds\": get_simplified_moods(mood_tags[:2]),\n",
    "                \"simplified_mood_gt\": r[\"mood\"],\n",
    "                \"genre_tags\": genre_tags,\n",
    "                \"genre_cs\": genre_cs,\n",
    "                \"simplified_genre_preds\": get_simplified_genres(genre_tags[:2]),\n",
    "                \"simplified_genre_gt\": r[\"genre\"],\n",
    "                \"instruments_preds\": [i.lower() for i in instrument_summary],\n",
    "                \"instruments_gt\": instrument_summary_gt,\n",
    "            }\n",
    "\n",
    "            annotation_f.write(json.dumps(row) + \"\\n\")\n",
    "            annotation_f.flush()\n",
    "\n",
    "            # if ind > 10:\n",
    "            #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b75c4-4fa2-47c7-b1d2-1d1b0b66f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "for approach in [InferenceApproach.FINETUNED_2]:\n",
    "# for approach in [InferenceApproach.PROMPT_ONLY, InferenceApproach.FINETUNED_2, InferenceApproach.RAG]:\n",
    "\n",
    "    print(approach)\n",
    "\n",
    "    ANNOTATION_OUTPUT_PATH = OUTPUT_FOLDER / f\"annotations_{approach.value}_output.jsonl\"\n",
    "    df = pd.read_json(ANNOTATION_OUTPUT_PATH, lines=True)\n",
    "\n",
    "    # genre\n",
    "    genre_gt_l = df[\"simplified_genre_gt\"].to_list()\n",
    "    genre_preds_l = df[\"simplified_genre_preds\"].to_list()\n",
    "    no_constraints_genre_preds_l = []\n",
    "    for i_gt, i_pred in zip(genre_gt_l, genre_preds_l):\n",
    "        if len(i_gt) > 0:\n",
    "            no_constraints_genre_preds_l.append(i_pred)\n",
    "        else:\n",
    "            no_constraints_genre_preds_l.append([])\n",
    "    genre_report = evaluate_prediction(\n",
    "        genre_classes, no_constraints_genre_preds_l, genre_gt_l\n",
    "    )\n",
    "\n",
    "    # mood\n",
    "    mood_gt_l = df[\"simplified_mood_gt\"].to_list()\n",
    "    mood_preds_l = df[\"simplified_mood_preds\"].to_list()\n",
    "    no_constraints_mood_preds_l = []\n",
    "    for i_gt, i_pred in zip(mood_gt_l, mood_preds_l):\n",
    "        if len(i_gt) > 0:\n",
    "            no_constraints_mood_preds_l.append(i_pred)\n",
    "        else:\n",
    "            no_constraints_mood_preds_l.append([])\n",
    "    mood_report = evaluate_prediction(\n",
    "        mood_classes, no_constraints_mood_preds_l, mood_gt_l\n",
    "    )\n",
    "\n",
    "    # instrument\n",
    "    instruments_gt_l = df[\"instruments_gt\"].to_list()\n",
    "    instruments_preds_l = df[\"instruments_preds\"].to_list()\n",
    "    no_constraints_instruments_preds_l = []\n",
    "    for i_gt, i_pred in zip(instruments_gt_l, instruments_preds_l):\n",
    "        if len(i_gt) > 0:\n",
    "            no_constraints_instruments_preds_l.append(i_pred)\n",
    "        else:\n",
    "            no_constraints_instruments_preds_l.append([])\n",
    "    instruments_report = evaluate_prediction(\n",
    "        instruments_classes,\n",
    "        no_constraints_instruments_preds_l,\n",
    "        instruments_gt_l\n",
    "    )\n",
    "    print(instruments_report)\n",
    "\n",
    "    print(f\"genre : {genre_report['micro avg']}\")\n",
    "    print(f\"mood  : {mood_report['micro avg']}\")\n",
    "    print(f\"inst  : {instruments_report['micro avg']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f5a5f-9e6c-4d33-be49-0868e2018668",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_prediction(mood_classes, [[], [\"calm\", \"emotional\", \"positive\"]], [[], [\"emotional\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add64c0-7a71-49ed-8970-78fc8ff18803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
